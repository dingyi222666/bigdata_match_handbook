## 大数据竞赛-构建数据仓库(二)

> 请查看需操作那些主机，默认情况下都将在这些主机里执行操作（环境中 master 作为客户端，slave1 作为服务器端，因此都需要使用到 hive。以下操作除特殊说明外都在master，slave1节点上操作）

### 提示

>通过给定安装包下载路径下载Hive安装包并解压到指定路径，配置Hadoop 相关配置文件，修改环境变量并生效。因为服务端需要和Mysql通信，服务端需要将Mysql的lib安装包放在和Hive的conf配置目录下。由于客户端需要和Hadoop通信，更改Hadoop中jline的版本，保留高版本
>
>1. 在master和slave1节点通过给定安装包下载路径下载hive安装包并解压到指定路径/usr/hive
>2. 在master客户端中更改Hadoop中jline的版本，将/usr/hive/apache-hive-2.1.1-bin/lib文件中的jline-2.12.jar拷贝到Hadoop中lib位置为/usr/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib。
>3. slave1中通过给定安装包下载路径下载mysql-connector-java-5.1.47-bin.jar到/usr/hive/apache-hive-2.1.1-bin/lib

> 在slave1上进行hive服务器端相关配置，将ConnectionURL指向mysql地址，并配置驱动名、以及数据库连接账号和密码。
>
> 1. 修改配置文件/usr/hive/apache-hive-2.1.1-bin/hive-site.xml，配置数据存储位置/user/hive_remote/warehouse
> 2. 修改/usr/hive/apache-hive-2.1.1-bin/hive-site.xml文件，配置连接Mysql，地址指向slave2：3306
> 3. 修改/usr/hive/apache-hive-2.1.1-bin/hive-site.xml文件，配置connector连接驱动com.mysql.jdbc.Driver
> 4. 修改/usr/hive/apache-hive-2.1.1-bin/hive-site.xml文件，配置connector连接用户名:root
> 5. 修改/usr/hive/apache-hive-2.1.1-bin/hive-site.xml文件，配置connector连接密码:123456

>在master上进行hive客户端相关配置，关闭本地模式，将hive.metastore.uris指向metastore服务器slave1的地址。
>
>1. 修改/usr/hive/apache-hive-2.1.1-bin/hive-site.xml文件，配置数据存储位置/user/hive_remote/warehouse
>2. 修改/usr/hive/apache-hive-2.1.1-bin/hive-site.xml文件，配置关闭local模式
>3. 修改/usr/hive/apache-hive-2.1.1-bin/hive-site.xml文件，配置远程连接到slave1服务器的9083端口

### 将Hive安装包解压到指定路径/usr/hive

```shell
mkdir /usr/hive
cd /usr/package ## 进入安装包文件夹
tar -zxvf apache-hive-2.1.1-bin.tar.gz -C /usr/hive

#slave1
scp -r /usr/hive root@slave1:/usr
```



### 设置Hive系统环境变量

```shell
echo -e 'export HIVE_HOME=/usr/hive/apache-hive-2.1.1-bin\nexport PATH=$PATH:$HIVE_HOME/bin' >> /etc/profile && source /etc/profile
# 看情况修改对应的路径
```



###  设置Hive运行环境



#### 修改hive-env.sh

```shell
cd /usr/hive/apache-hive-2.1.1-bin/conf # 移动到配置环境
cp hive-env.sh.template hive-env.sh # 复制模板文件
echo $HADOOP_HOME # 查看hadoop路径 等会需要用到
vim hive-env.sh # 编辑env文件
```

更改如下配置代码

```shell
export HADOOP_HOME=/usr/hadoop/hadoop-2.7.3
export HIVE_CONF_DIR=/usr/hive/apache-hive-2.1.1-bin/conf
export HIVE_AUX_JARS_PATH=/usr/hive/apache-hive-2.1.1-bin/lib
```

保存 并使用scp

```shell
scp hive-env.sh root@slave1:/usr/hive/apache-hive-2.1.1-bin/conf
```



#### 新建hive-site.xml

```shell
# 使用如下语句打开并新建hive-site.xml
vim hive-site.xml
```



在slave1上添加如下内容

```xml
<configuration>
  <!--Hive产生的元数据存放位置-->
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive_remote/warehouse</value>
</property>
    <!--数据库连接JDBC的URL地址-->
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://slave2:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
	<description>JDBC connect string for a JDBC metastore</description>
</property>
    <!--数据库连接driver，即MySQL驱动-->
<property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
</property>
    <!--MySQL数据库用户名-->
<property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>root</value>
</property>
    <!--MySQL数据库密码-->
<property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>123456</value>
 </property>
<property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
 </property>
<property>
    <name>datanucleus.schema.autoCreateALL</name>
    <value>true</value>
 </property>
</configuration>
```



在master上添加如下内容

```xml
<configuration>
<!--Hive产生的元数据存放位置-->
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive_remote/warehouse</value>
</property>
    
<!---使用本地服务连接Hive，默认为true-->
<property>
    <name>hive.metastore.local</name>
    <value>false</value>
</property>
 
<!--连接服务器-->
<property>
    <name>hive.metastore.uris</name>
    <value>thrift://slave1:9083</value>
</property>
</configuration>
```

保存即可

#### 复制mysql依赖jar

由于服务端（slave1)需要和slave2作通讯，需要mysql jdbc包做驱动支持

> 只在salve1作操作。

```shell
cd /usr/package ## 跳转到软件包文件夹
cp mysql-connector-java-5.1.47-bin.jar /usr/hive/apache-hive-2.1.1-bin/lib
```



###  解决Jline的版本冲突

> 将hive安装目录下的lib目录下的jline复制至hadoop目录下，具体操作如下
>
> 提示：只需要在master上操作

```shell
cd /usr/hive/apache-hive-2.1.1-bin/lib && cp jline-2.12.jar /usr/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib
```



### 启动Hive

#### 服务器端初始化数据库，启动metastore服务

##### 启动hadoop集群

>在master上操作

```shell
cd /usr/hadoop/hadoop-2.7.3 && start-dfs.sh && start-yarn.sh  
```



##### 初始化数据库

> 在slave1上操作

```shell
schematool -dbType mysql -initSchema
```

出现以下界面则为初始化成功

![](https://img-blog.csdnimg.cn/20210904161258934.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNTMzODMzNDk=,size_20,color_FFFFFF,t_70,g_se,x_16)

然后继续

```shell
cd /usr/hive/apache-hive-2.1.1-bin && bin/hive --service metastore
```

出现以下界面则为初始化成功

![img](https://img-blog.csdnimg.cn/20210904223543156.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNTMzODMzNDk=,size_20,color_FFFFFF,t_70,g_se,x_16)

> 在master上操作

```shell
cd /usr/hive/apache-hive-2.1.1-bin && bin/hive
```

hive启动成功后，输入命令`show databases;`，出现如下界面则为成功~~~

![](https://img-blog.csdnimg.cn/20210904164758735.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNTMzODMzNDk=,size_20,color_FFFFFF,t_70,g_se,x_16)



#### 在Hive客户端下下创建数据库student

```mysql
create database student;
```

